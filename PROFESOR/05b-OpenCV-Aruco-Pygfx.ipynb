{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6610186-c67e-4959-b6f6-70f8da6db078",
   "metadata": {},
   "source": [
    "# Render de modelos 3D en Realidad Aumentada con Pygfx\n",
    "\n",
    "[Pygfx](https://pygfx.org/) es una biblioteca de Python para el render de escenas 3D que implementa [PBR (Physically Based Rendering)](https://en.wikipedia.org/wiki/Physically_based_rendering) y capaz de interpretar modelos en formato [glTF 2.0](https://www.khronos.org/gltf/) propuesto por el grupo [Khronos](https://www.khronos.org/), responsable de múltiples estándares abiertos.\n",
    "\n",
    ">     pip install pygfx\n",
    "\n",
    "\n",
    "\n",
    ">     pip install gltflib\n",
    ">     pip install imageio   #dependencia de gltflib\n",
    "\n",
    "Pygfx ofrece una API muy simple de usar para la carga y render de modelos, a la vez que presenta un rendimiento y calidad destacados. La composición de la escena en Pygfx la vamos a realizar con las clases escenaPYGFX y modeloGLTF definidos en cuia.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceb8177-5ed9-4532-8b5e-6fb0b271eeb9",
   "metadata": {},
   "source": [
    "El renderizado de una escena 3D necesita:\n",
    "* Uno o varios modelos 3D.\n",
    "* Una o varias luces, imprescindibles para que se pueda ver el modelo. Una de las luces puede ser una luz ambiental omnipresente que no necesita que se indique su ubicación.\n",
    "* Una cámara en la que se hará la proyección de la escena.\n",
    "\n",
    "Modelos, luces y cámara deben estar ubicados en la escena en ubicaciones y posiciones relativas a un origen de coordenadas. En Realidad Aumentada, el origen de coordenadas de la escena lo ubicaremos en el centro del marcador Aruco y la cámara estará en la posición relativa de la webcam con respecto al marcador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2e39e9-0f90-4e45-9fd1-aee7a6f5a7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import cuia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f707b2-11f5-4998-a48a-c21749b07278",
   "metadata": {},
   "source": [
    "Cargamos los parámetros de la cámara (el id lo definimos en la variable cam) desde el fichero generado por el calibrado [camara.py](camara.py). Si no existe entonces definimos unos parámetros de una cámara ideal sin distorsiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386d616b-7cbd-4216-ac66-a9c167730001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = 0\n",
    "bk = cuia.bestBackend(cam)\n",
    "\n",
    "# Obtenemos el alto y ancho de los frames capturados por la cámara\n",
    "webcam = cv2.VideoCapture(cam,bk)\n",
    "ancho = int(webcam.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "alto = int(webcam.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "webcam.release()\n",
    "\n",
    "try:\n",
    "    # Importamos la matriz característica de la cámara y sus coeficientes de distorsión del fichero de calibrado\n",
    "    import camara\n",
    "    cameraMatrix = camara.cameraMatrix\n",
    "    distCoeffs = camara.distCoeffs\n",
    "except ImportError:\n",
    "    # Si la cámara no estaba calibrada suponemos que no presenta distorsiones\n",
    "    cameraMatrix = np.array([[ 1000,    0, ancho/2],\n",
    "                             [    0, 1000,  alto/2],\n",
    "                             [    0,    0,       1]])\n",
    "    distCoeffs = np.zeros((5, 1)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b43b46-dc29-4822-9865-d9df7dc06a37",
   "metadata": {},
   "source": [
    "La localización, rotación y escala de los elementos de la escena 3D se especifica mediante [matrices de transformación](https://es.wikipedia.org/wiki/Matriz_de_transformaci%C3%B3n), que representan dichas operaciones en forma de matrices 4x4. Para la creación y composición de dichas matrices utilizaremos la clase matrizDeTransformacion definida en [cuia.py](cuia.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ca9c3-bd28-48fc-b56c-702b42bb8259",
   "metadata": {},
   "source": [
    "En una escena necesitamos 3 elementos\n",
    "* Uno o más modelos 3D ubicados en algún punto del espacio con respecto a un eje de coordenadas de la escena.\n",
    "* Una o más fuentes de iluminación.\n",
    "* Una cámara que realizará la proyección 2D de la escena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc7a222-c68d-4ed5-9cd8-d2853da30dd9",
   "metadata": {},
   "source": [
    "Hay muchos formatos de representación de modelos 3D. Por ahora solo consideraré el formato [glTF (GL Transmission Format)](https://es.wikipedia.org/wiki/GlTF) propuesto por el grupo [Khronos](https://es.wikipedia.org/wiki/Khronos_Group). Emplearé la clase modeloGLTF definida en [cuia.py](cuia.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0817d8c-ad47-448d-b3bb-aba888e9b13f",
   "metadata": {},
   "source": [
    "El objetivo es mezclar la imagen del mundo real que captura la webcam con la imagen del mundo virtual que captura la cámara de la escena. Para que esta mezcla sea correcta es necesario que ambas cámaras tengan las mismas características. En el caso de Pygfx utilizaremos una [PerspectiveCamera](https://docs.pygfx.org/stable/_autosummary/cameras/pygfx.cameras.PerspectiveCamera.html#pygfx.cameras.PerspectiveCamera) que necesita que indiquemos el ancho y alto de la imagen capturada y el [campo de visión (FoV)](https://es.wikipedia.org/wiki/Campo_de_visi%C3%B3n) expresado en grados, de la menos de estas dimensiones, que suele ser la vertical. Para calcular el FoV necesitamos la matriz característica de la cámara que ya tendremos si hicimos previamente el [calibrado](04-OpenCV-Calibrado.ipynb) de la cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e715e540-a874-4bf1-9f51-9501aa3b9b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = cuia.modeloGLTF('../media/pera.glb')\n",
    "modelo.rotar((np.pi/2.0, 0, 0)) # Rotar el modelo 90 grados en X para que coincida con el punto de vista que se obtiene en Blender\n",
    "modelo.escalar(0.15) # Escalado uniforme del modelo\n",
    "modelo.flotar() # Sitúa el modelo en el rango positivo del eje Z\n",
    "# Reproducimos las animaciones que haya en el modelo\n",
    "lista_animaciones = modelo.animaciones()\n",
    "if len(lista_animaciones) > 0:\n",
    "    modelo.animar(lista_animaciones[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ebbb05-56d7-4e69-a268-cde444082a4b",
   "metadata": {},
   "source": [
    "**Nota**: el sistema de coordenadas usado por Pygfx (el mismo que usa OpenGL) tiene una orientación distinta al que usa OpenCV. \n",
    "\n",
    "![Sistems de coordenadas de OpenCV y Pygfx](media/opencvpygfx.png \"Sistems de coordenadas de OpenCV y Pygfx\")\n",
    "\n",
    "Por ello será necesario hacer la conversión adecuada para el uso combinado de OpenCV y Pygfx.\n",
    "\n",
    "Dada la pose percibida del marcador y calculada mediante [solvePnP](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga549c2075fac14829ff4a58bc931c033d), que tendremos especificada en forma de un vector de translación **tvec** y un vector de rotación **rvec**, necesitamos expresarla en forma de matriz de transformación y adaptarla al modelo de sistema de coordenadas de Pygfx. Esto implicará el uso de la función [Rodrigues](https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga61585db663d9da06b68e70cfbf6a1eac) y la inversión de las componentes Y y Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc97f170-f456-458b-a1ae-5ac9325a5b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fromOpencvToPygfx(rvec, tvec):\n",
    "    pose = np.eye(4)\n",
    "    pose[0:3,3] = tvec.T\n",
    "    pose[0:3,0:3] = cv2.Rodrigues(rvec)[0]\n",
    "    pose[1:3] *= -1  # Inversión de los ejes Y y Z\n",
    "    pose = np.linalg.inv(pose)\n",
    "    return(pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153393c5-c5a0-4304-ac67-8057b7882d09",
   "metadata": {},
   "source": [
    "Esta función será la que nos indique la matriz de transformación empleada para ubicar la cámara en Pygfx (en la misma posición relativa de la webcam con respecto al marcador). El resto de elementos de la escena los ubicaremos en una posición relativa al origen de coordenadas ubicado en el centro del marcador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9400ef3-7618-46ca-895e-abf879934f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fov(cameraMatrix, ancho, alto):\n",
    "    if ancho > alto:\n",
    "        f = cameraMatrix[1, 1]\n",
    "        fov_rad = 2 * np.arctan(alto / (2 * f))\n",
    "    else:\n",
    "        f = cameraMatrix[0, 0]\n",
    "        fov_rad = 2 * np.arctan(ancho / (2 * f))\n",
    "    return np.rad2deg(fov_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba453788-5ea9-4225-816e-b833a5d3c655",
   "metadata": {},
   "source": [
    "Usaremos la clase escenaPYGFX para crear una nueva escena y añadir el modelo e iluminación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41d0d3-4e2b-4491-836b-02b2de7bb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "escena = cuia.escenaPYGFX(fov(cameraMatrix, ancho, alto), ancho, alto)\n",
    "escena.agregar_modelo(modelo)\n",
    "escena.ilumina_modelo(modelo)\n",
    "escena.iluminar() # Agrega iluminación ambiental"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5ab64-d418-434a-99ae-a68995f82300",
   "metadata": {},
   "source": [
    "La clase escenaPYGFX agrega automáticamente la cámara a la escena. La posición y orientación de la cámara se especificará mediante una [matrizDeTransformacion](cuia.py) que se irá actualizando en función de la pose detectada del marcador de Aruco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b670a3ce-7ad9-4d16-89cd-3e5dee20b9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = cuia.myVideo(cam, bk) # Iniciamos un objeto myVideo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a00bd-fa8b-47c9-a985-cbfd7fd0957f",
   "metadata": {},
   "source": [
    "Para cada frame debemos actualizar la cámara en función de la detección del marcador Aruco. Utilizaremos diccionario 5X5_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99935ae4-b755-4a19-a6d6-a14a81001266",
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_5X5_50)\n",
    "detector = cv2.aruco.ArucoDetector(diccionario)\n",
    "\n",
    "def detectarPose(frame, tam): # tam es el tamaño en metros del marcador\n",
    "    bboxs, ids, rechazados = detector.detectMarkers(frame)\n",
    "    print(\"ids: \",ids)\n",
    "    if ids is not None:\n",
    "        print(\"ha entrado al if\")\n",
    "        objPoints = np.array([[-tam/2.0, tam/2.0, 0.0],\n",
    "                              [tam/2.0, tam/2.0, 0.0],\n",
    "                              [tam/2.0, -tam/2.0, 0.0],\n",
    "                              [-tam/2.0, -tam/2.0, 0.0]])\n",
    "        resultado = {}\n",
    "        for i in range(len(ids)):\n",
    "                ret, rvec, tvec = cv2.solvePnP(objPoints, bboxs[i], cameraMatrix, distCoeffs)\n",
    "                if ret:\n",
    "                    resultado[ids[i][0]] = (rvec, tvec)\n",
    "        return((True, resultado))\n",
    "    return((False, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145386b6-6fe6-42a5-a232-483dfacae224",
   "metadata": {},
   "source": [
    "La función realidadMixta será la que procese cada frame, actualice la cámara, obtenga el render de la escena y combine la imagen obtenida con el frame de la cámara."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1c10f67-6feb-4fc1-8484-5d08aa17d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def realidadMixta(frame):\n",
    "    ret, pose = detectarPose(frame, 0.19)\n",
    "    if ret and pose[0]:\n",
    "        M = fromOpencvToPygfx(pose[0][0], pose[0][1])\n",
    "        escena.actualizar_camara(M)\n",
    "        imagen_render = escena.render()\n",
    "        imagen_render_bgr = cv2.cvtColor(imagen_render, cv2.COLOR_RGBA2BGRA)\n",
    "        resultado = cuia.alphaBlending(imagen_render_bgr, frame)\n",
    "    else:\n",
    "        resultado = frame\n",
    "        \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b167c5-fa6b-4d9c-b71d-8edc5a48640b",
   "metadata": {},
   "source": [
    "En definitiva el proceso que ha de realizarse para cada frame consta de los siguientes pasos:\n",
    "* Detectar el marcador y obtener la pose (translación y rotación)\n",
    "* Ubicar la cámara de la escena 3D en la posición de la webcam\n",
    "* Combinar el renderizado del modelo 3D con la imagen de la webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f73af05e-4e26-4c03-8410-074d3969eac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n",
      "[[0]]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ar.process = realidadMixta\n",
    "try:\n",
    "    ar.play(\"AR\", key=ord(' '))\n",
    "finally:\n",
    "    ar.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07fad55-bb92-47c3-978b-ffc4dc9c48a6",
   "metadata": {},
   "source": [
    "![Prueba de render](media/rendertierra.png \"Prueba de render\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
